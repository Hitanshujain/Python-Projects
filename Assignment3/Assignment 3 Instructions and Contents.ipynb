{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlGsu74PRze0"
   },
   "source": [
    "[<h1> FIT5197 Assignment 3 Semester 2, 2020 </h1>](https://lms.monash.edu/mod/assign/view.php?id=7560449)\n",
    "\n",
    "---\n",
    "Authors: Dan Nguyen, Yun Zhao\n",
    "\n",
    "Admins (Competition): Dr. Levin Kuhlmann, Yun Zhao, Anil Gurbuz\n",
    "\n",
    "Proofreaders: Dr. Levin Kuhlmann, Yun Zhao, and other tutors \n",
    "\n",
    "Date: Oct 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h1> Assignment Instruction </h1>](https://lms.monash.edu/mod/assign/view.php?id=7560449)\n",
    "\n",
    "<span style=\"color:red\"> Please read through the instructions carefully, by submitting the assignment, you are considered to have read all the instructions carefully and be aware of the penalties that entail. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "<h1>Part 1: Regression (50 Marks)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar--DpysOWJd"
   },
   "source": [
    "This part is about regression. Specifically, you will be ``predicting the fuel efficiency`` of a car (in kilometers per litre) based on its characteristics. This is a practical problem as Australia is one of the largest automobile markets in the world; thus, correctly predicting the fuel efficiency is necessary to control emission rates to the environment.\n",
    "\n",
    "The dataset has many observations and predictors obtained from many retailers for car models available for sale from 2017 to 2020. The target variable is the fuel efficiency of the car measured in kilometers per litre. The higher this value, the better the fuel efficiency of the car. \n",
    "\n",
    "PleaseProvide working/R code/justifications for each of these questions as required.\n",
    "\n",
    "$\\textbf{Note:}$ If not explicitly mentioned, libraries are not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"RegressionTrain.csv\")\n",
    "test <- read.csv(\"RegressionTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/RegressionTrain.csv\")\n",
    "test <- read.csv(\"../data/RegressionTest.csv\")\n",
    "label <- read.csv(\"../data/RegressionTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "<h2> Question 1 (5 Marks) </h2>\n",
    "\n",
    "Fit a $\\textbf{multiple linear model}$ to the fuel efficiency data using the ``train`` dataset. By checking the summary information, which predictors/variables do you think are possibly associated with fuel efficiency (use ``0.05`` significant level), and why? Which ``three predictors/variables`` appear to be the strongest predictors of fuel efficiency, and why? \n",
    "\n",
    "$\\textbf{Note}$: You don't have to worry about categorical variables here since R can deal with this automatically, focus your efforts on interpretation. Additionally, when explaining why features are strongly associated with the target, please refrain giving one or two sentences answers, these answers are not descriptive enough and will result in deduction of marks. Finally, please name the model here ``lm.fit`` for future marking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ ., data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0256 -0.9978 -0.0644  0.7006 11.3941 \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              -1.783e+02  8.587e+01  -2.076  0.03809 *  \n",
       "Model.Year                9.640e-02  4.255e-02   2.266  0.02363 *  \n",
       "Eng.Displacement         -1.364e+00  1.025e-01 -13.306  < 2e-16 ***\n",
       "No.Cylinders              4.644e-02  6.769e-02   0.686  0.49282    \n",
       "AspirationOT             -3.452e-01  6.352e-01  -0.543  0.58693    \n",
       "AspirationSC             -9.197e-01  2.282e-01  -4.031 5.85e-05 ***\n",
       "AspirationTC             -1.303e+00  1.288e-01 -10.111  < 2e-16 ***\n",
       "AspirationTS             -1.149e+00  4.945e-01  -2.323  0.02035 *  \n",
       "No.Gears                 -1.307e-01  2.995e-02  -4.364 1.37e-05 ***\n",
       "Lockup.Torque.ConverterY -8.243e-01  1.117e-01  -7.377 2.78e-13 ***\n",
       "Drive.SysA               -8.339e-02  1.521e-01  -0.548  0.58356    \n",
       "Drive.SysF                1.441e+00  1.711e-01   8.419  < 2e-16 ***\n",
       "Drive.SysP               -2.400e-01  2.980e-01  -0.805  0.42087    \n",
       "Drive.SysR                4.328e-02  1.476e-01   0.293  0.76938    \n",
       "Max.Ethanol              -7.076e-03  2.967e-03  -2.385  0.01722 *  \n",
       "Fuel.TypeGM               5.706e-01  4.173e-01   1.368  0.17169    \n",
       "Fuel.TypeGP               4.093e-01  1.369e-01   2.990  0.00284 ** \n",
       "Fuel.TypeGPR              1.363e-01  1.401e-01   0.973  0.33096    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.598 on 1382 degrees of freedom\n",
       "Multiple R-squared:  0.6628,\tAdjusted R-squared:  0.6586 \n",
       "F-statistic: 159.8 on 17 and 1382 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mutlivariate linear regression model \n",
    "lm.fit <- lm(Comb.FE ~ .,data=train)\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in TopPredictor(lm.fit): could not find function \"TopPredictor\"\n",
     "output_type": "error",
     "traceback": [
      "Error in TopPredictor(lm.fit): could not find function \"TopPredictor\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "TopPredictor.lm <- function(model){\n",
    "    # Finding the best predictors/variables for fuel efficiency\n",
    "    Predictor <- coef(summary(model))\n",
    "    # Filtering only predictors/variables with significance of 0.05\n",
    "    Best.Pred <- Predictor[Predictor[,\"Pr(>|t|)\"]<0.05,]\n",
    "    Predictor <- sort(Best.Pred[,4],decreasing=FALSE)\n",
    "    print(Predictor[1])\n",
    "    print(Predictor[2])\n",
    "    print(Predictor[3])\n",
    "    }\n",
    "\n",
    "TopPredictor(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<span style=\"color:blue\">SOLUTION 1:</span>__ \n",
    "\n",
    "__<span style=\"color:blue\">Predictor/variables (significance level of 0.05 or less) that have possibly associated with fuel efficiency are:- </span>__\n",
    "\n",
    "1. Model.Year           &nbsp;        ``Significance: 0.05``\n",
    "2. Eng.Displacement     &nbsp;        ``Significance: 0.0``\n",
    "3. AspirationSC         &nbsp;        ``Significance: 0.0``\n",
    "4. AspirationTC         &nbsp;        ``Significance: 0.0``\n",
    "5. AspirationTS         &nbsp;        ``Significance: 0.0``\n",
    "6. No.Gears             &nbsp;        ``Significance: 0.0``\n",
    "7. Lockup.Torque.ConverterY   &nbsp;  ``Significance: 0.0``\n",
    "8. Drive.SysF                &nbsp;   ``Significance: 0.0``\n",
    "9. Max.Ethanol                &nbsp;  ``Significance: 0.05``\n",
    "10. Fuel.TypeGP               &nbsp;  ``Significance: 0.0``\n",
    "\n",
    "Above variables have $\\textbf{Pr(>|t|)}$ less than or equal to 0.05 when applied Linear model function. \n",
    "\n",
    "__<span style=\"color:blue\">Predictor/variables with strongest associated with fuel efficiency are:- </span>__\n",
    "\n",
    "1. Eng.Displacement :    &nbsp;        $\\textbf{Pr(>|t|)} ---->$ less than 2e-16\n",
    "2. AspirationSC     :    &nbsp;        $\\textbf{Pr(>|t|)} ------>$ less than 2e-16\n",
    "3. Drive.SysF       :    &nbsp;   $\\textbf{Pr(>|t|)} ------->$ less than 2e-16\n",
    "\n",
    "These are the most significant and have the least $\\textbf{Pr(>|t|)}$ value when we apply linear modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "<h2> Question 2 (5 Marks) </h2>\n",
    "\n",
    "Describe/discuss the effect that the year of manufacture ``(Model.Year)`` variable appears to have on the mean ``fuel efficiency``. Additionally, describe/discuss the effect that the number of gears ``(No.Gears)`` variable has on the mean ``fuel efficiency`` of the car.\n",
    "\n",
    "$\\textbf{Note}$: This asks for your descriptions, please refrain from using one or two lines to describe/discuss the effect. Keep answers to be 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 2:</span>__ \n",
    "\n",
    "__<span style=\"color:blue\">Model.Year versus mean fuel efficiency</span>__\n",
    "\n",
    "From below graph we can conclude that model year don't have any significant effect on fuel efficiency\n",
    "\n",
    "__<span style=\"color:blue\">Numbers of Gears versus mean fuel efficiency</span>__\n",
    "\n",
    "From below graph we can conclude that Numbers of Gears does have significant effect on fuel efficiency. As No. of gears increases Fuel efficiency of mean car decreases significantly. Although not perfect an linear equation can be generated that might show graphs behaviour  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nCUAARAIQAJEABPgPxLGrH/SsExIAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Model.Year (Year) versus mean fuel efficiency (x)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Numbers of Gears (No.Gears) versus mean fuel efficiency (x)\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grouping and finding the mean of data\n",
    "df1 <- aggregate(x = train$Comb.FE,by=list(Year=train$Model.Year),FUN=mean)\n",
    "df2 <- aggregate(x = train$Comb.FE,by=list(No.Gears=train$No.Gears),FUN=mean)\n",
    "\n",
    "# Plotting graph\n",
    "plot(df1,col  = \"red\",main = \"Model.Year (Year) versus mean fuel efficiency (x)\")\n",
    "lines((df1))\n",
    "plot(df2,col  = \"red\",main = \"Numbers of Gears (No.Gears) versus mean fuel efficiency (x)\")\n",
    "lines((df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "<h2> Question 3 (5 Marks) </h2>\n",
    "\n",
    "Apply the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially less significant variables. Write down the final regression equation obtained after pruning, please keep the values of the parameter coefficients to 2 decimal places. Finally, also describe the pruned model.\n",
    "\n",
    "$\\textbf{Note}$: please don't change the default direction ``both`` in the step function, this is so that we can check your work easily. Additionally, please name this model ``sw.fit``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "$\\textbf{YOUR ANSWER HERE}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=1424.68\n",
      "Comb.FE ~ Model.Year + Eng.Displacement + No.Cylinders + Aspiration + \n",
      "    No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol + \n",
      "    Fuel.Type\n",
      "\n",
      "                          Df Sum of Sq    RSS    AIC\n",
      "- Fuel.Type                3     27.87 3556.7 1414.0\n",
      "- No.Cylinders             1      1.20 3530.0 1417.9\n",
      "- Model.Year               1     13.11 3541.9 1422.6\n",
      "- Max.Ethanol              1     14.52 3543.3 1423.2\n",
      "<none>                                 3528.8 1424.7\n",
      "- No.Gears                 1     48.63 3577.4 1436.6\n",
      "- Lockup.Torque.Converter  1    138.97 3667.8 1471.5\n",
      "- Aspiration               4    267.94 3796.7 1498.2\n",
      "- Drive.Sys                4    393.01 3921.8 1543.5\n",
      "- Eng.Displacement         1    452.08 3980.9 1586.2\n",
      "\n",
      "Step:  AIC=1413.96\n",
      "Comb.FE ~ Model.Year + Eng.Displacement + No.Cylinders + Aspiration + \n",
      "    No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol\n",
      "\n",
      "                          Df Sum of Sq    RSS    AIC\n",
      "- No.Cylinders             1      1.68 3558.4 1407.4\n",
      "- Model.Year               1     14.18 3570.9 1412.3\n",
      "<none>                                 3556.7 1414.0\n",
      "- Max.Ethanol              1     18.68 3575.4 1414.0\n",
      "- No.Gears                 1     42.25 3598.9 1423.2\n",
      "+ Fuel.Type                3     27.87 3528.8 1424.7\n",
      "- Lockup.Torque.Converter  1    144.88 3701.6 1462.6\n",
      "- Aspiration               4    266.66 3823.3 1486.2\n",
      "- Drive.Sys                4    384.97 3941.6 1528.9\n",
      "- Eng.Displacement         1    449.79 4006.5 1573.4\n",
      "\n",
      "Step:  AIC=1407.38\n",
      "Comb.FE ~ Model.Year + Eng.Displacement + Aspiration + No.Gears + \n",
      "    Lockup.Torque.Converter + Drive.Sys + Max.Ethanol\n",
      "\n",
      "                          Df Sum of Sq    RSS    AIC\n",
      "- Model.Year               1     13.78 3572.1 1405.5\n",
      "<none>                                 3558.4 1407.4\n",
      "- Max.Ethanol              1     18.74 3577.1 1407.5\n",
      "+ No.Cylinders             1      1.68 3556.7 1414.0\n",
      "- No.Gears                 1     43.44 3601.8 1417.1\n",
      "+ Fuel.Type                3     28.35 3530.0 1417.9\n",
      "- Lockup.Torque.Converter  1    146.78 3705.1 1456.7\n",
      "- Aspiration               4    295.13 3853.5 1490.0\n",
      "- Drive.Sys                4    387.17 3945.5 1523.0\n",
      "- Eng.Displacement         1   2217.81 5776.2 2078.3\n",
      "\n",
      "Step:  AIC=1405.54\n",
      "Comb.FE ~ Eng.Displacement + Aspiration + No.Gears + Lockup.Torque.Converter + \n",
      "    Drive.Sys + Max.Ethanol\n",
      "\n",
      "                          Df Sum of Sq    RSS    AIC\n",
      "<none>                                 3572.1 1405.5\n",
      "- Max.Ethanol              1     20.82 3593.0 1406.4\n",
      "+ Model.Year               1     13.78 3558.4 1407.4\n",
      "+ No.Cylinders             1      1.28 3570.9 1412.3\n",
      "- No.Gears                 1     38.98 3611.1 1413.5\n",
      "+ Fuel.Type                3     29.35 3542.8 1415.7\n",
      "- Lockup.Torque.Converter  1    144.44 3716.6 1453.8\n",
      "- Aspiration               4    298.65 3870.8 1489.0\n",
      "- Drive.Sys                4    382.97 3955.1 1519.2\n",
      "- Eng.Displacement         1   2228.54 5800.7 2077.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ Eng.Displacement + Aspiration + No.Gears + \n",
       "    Lockup.Torque.Converter + Drive.Sys + Max.Ethanol, data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0743 -0.9760 -0.0349  0.6566 11.3971 \n",
       "\n",
       "Coefficients:\n",
       "                          Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              16.196874   0.282901  57.253  < 2e-16 ***\n",
       "Eng.Displacement         -1.277173   0.043418 -29.416  < 2e-16 ***\n",
       "AspirationOT             -0.100081   0.626276  -0.160 0.873060    \n",
       "AspirationSC             -0.699137   0.213768  -3.271 0.001100 ** \n",
       "AspirationTC             -1.144227   0.107302 -10.664  < 2e-16 ***\n",
       "AspirationTS             -1.122104   0.481471  -2.331 0.019919 *  \n",
       "No.Gears                 -0.113537   0.029183  -3.891 0.000105 ***\n",
       "Lockup.Torque.ConverterY -0.825285   0.110202  -7.489 1.23e-13 ***\n",
       "Drive.SysA                0.035013   0.145617   0.240 0.810020    \n",
       "Drive.SysF                1.480191   0.166847   8.872  < 2e-16 ***\n",
       "Drive.SysP               -0.323201   0.292617  -1.105 0.269560    \n",
       "Drive.SysR                0.093779   0.146329   0.641 0.521710    \n",
       "Max.Ethanol              -0.008344   0.002934  -2.843 0.004528 ** \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 1.605 on 1387 degrees of freedom\n",
       "Multiple R-squared:  0.6586,\tAdjusted R-squared:  0.6557 \n",
       "F-statistic:   223 on 12 and 1387 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mutlivariate linear regression model with pruning done using BIC penalty\n",
    "sw.fit <- step(lm.fit,k = log(nrow(train)), direction=\"both\")\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<span style=\"color:blue\">SOLUTION 3:</span>__ \n",
    "\n",
    "BIC penalty can help Prune variables which doesn't impact fuel efficency significantly. Model add or remove variables to give the equation with minimum AIC.\n",
    "\n",
    "<span style=\"color:blue\">Step 1:</span> ``AIC=1424.68`` \n",
    "\n",
    "Comb.FE ~ Model.Year + Eng.Displacement + No.Cylinders + Aspiration + No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol + Fuel.Type\n",
    "\n",
    "<span style=\"color:blue\">Step 2:</span> ``AIC=1413.96`` \n",
    "\n",
    "Comb.FE ~ Model.Year + Eng.Displacement + No.Cylinders + Aspiration + No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol\n",
    "\n",
    "<span style=\"color:blue\">Step 3:</span> ``AIC=1407.38`` \n",
    "\n",
    "Comb.FE ~ Model.Year + Eng.Displacement + Aspiration + No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol \n",
    "\n",
    "<span style=\"color:blue\">Step 4:</span> ``AIC=1405.54`` \n",
    "\n",
    "Comb.FE ~ Eng.Displacement + Aspiration + No.Gears + Lockup.Torque.Converter + Drive.Sys + Max.Ethanol\n",
    "\n",
    "\n",
    "<span style=\"color:blue\">FORMULA:</span>\n",
    "<b>Comb.FE = 16.2 - 1.28*Eng.displacement - 0.1*AspirationOT - 0.7*AspirationSC -1.14*AspiratioTC - 1.12*AspirationTS - 0.11*No.Gears -0.83*Lockup.Torque.Converter + 0.04*Drive.SysA + 1.48*Drive.SysF -0.32*Drive.SysP + 0.09* Drive.SysR - 0.01*Max.Ethanol<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izq8RU0lOWJn"
   },
   "source": [
    "<h2> Question 4 (5 Marks) </h2>\n",
    "\n",
    "Say we are going to buy a new car and we want to improve the fuel efficiency of our new car, what does this ``BIC model`` suggest we should do? Provide a detailed answers of at least ``150 words``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 4:</span>__ \n",
    "\n",
    "From the data formula mentioned above we get some sort of picture about an mathermatical model than can be used to figure out dependence of fuel efficiency on independent variable such as Engine displacement, Aspiration, Number of Gears ,Lockup Torque Converter , Drive System and Maximum Ethanol. These independent varaibles are selected by BIC model because there combination leads to lowest Akaike information criterion, i.e. AIC . Decreasing the values with negative coefficent such as Eng.displacement (coeff = -1.28) and increasing values with positive coefficent such as Drive.SysF (coeff = 1.48) is necceasry to improve the Fuel efficiency of our new car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhFKnrlcOWJo"
   },
   "source": [
    "<h2> Question 5 (5 Marks)</h2>\n",
    "\n",
    "Imagine that you are looking for a new car to buy to replace your existing car. Use the $\\textbf{test}$ dataset to inspect the first car fuel efficiency and see whether it is a good fit for you or not.\n",
    "    \n",
    "    (a) Use your BIC model to predict the mean fuel efficiency for this new car. Provide a 95% confidence interval for this prediction. [2 mark]\n",
    "    (b) Following the previous estimation, given that the current car that you own has a mean fuel efficiency of 9.5 km/l (measured over the life time of your ownership), does your model (BIC) suggest that the new car will have better fuel efficiency than your current car? Why? [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fit      lwr      upr \n",
      "9.287257 9.052956 9.521557 \n"
     ]
    }
   ],
   "source": [
    "# fuel efficiency prediction with 95% confidence interval\n",
    "test$Comb.FE = predict(sw.fit,test,level=0.95, interval='confidence')\n",
    "print(test$Comb.FE[1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 5:</span>__ \n",
    "\n",
    "1. Using BIC (95% confidence interval) car's mean F.E. value is 9.287 , lower F.E. value is 9.053 and upper F.E. value 9.521\n",
    "\n",
    "2. As the value is very close to mean fuel efficiency of 9.5 km/l therefore it's not wise to buy current car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 6 (Libraries are allowed) (25 Marks) </h2>\n",
    "\n",
    "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limitted to these steps in the assignment. To simulate for a realistic modelling process, this question will be in the form of a competition among students to find out who has the best model.\n",
    "\n",
    "Thus, You will be graded by the performance of your model compared to your classmates', the better your model, the higher your score. Additionally, you need to write a short paragraph describing/documenting your thought process in this model building process ``(300 words)``. Note that this is to explain to us why you build your current model so that we can verify that you understand the model you build and not just copy from other people.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "``install.packages(\"some package\", repos='http://cran.us.r-project.org')``\n",
    "\n",
    "``library(\"some package\")``\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you 0 marks, our suggestion is for you to use the standard ``R version 3.6.1``\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works Appropriately.\n",
    "\n",
    "$20$ Marks for the model performance in the competition\n",
    "\n",
    "$5$ Marks for logically writing down the thought process in building the final model\n",
    "\n",
    "This is the [link](https://www.kaggle.com/t/0a3c0fc91b074816a6315bb4e9b42602) to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 6:</span>__ \n",
    "\n",
    "After pruning the data using BIC criterion, the R square value has observed to be 0.65. Hence, to improve the accuracy (RMSE score) first we need to \n",
    "1. transform the independent variables \n",
    "2. Find Optimum Regression model.\n",
    "\n",
    "ggpairs was used to observe the trend of fuel efficiency w.r.t other independent variables. To make an optimum final regression equation an combination of logs and polynomials of independent variables were used.\n",
    "After carefully observing the trend of fuel efficiency w.r.t other independent variables, \n",
    "\n",
    "Further randomForest modelling were used as final model as in comparision with other models tried, RandomForest resulted in better score when checked for output on kaggle. After that hyper parameter optimization were neccesary to extract best RMSE score.\n",
    "\n",
    "After multiple experiments combination of parameters were found which result in best score so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Library used\n",
    "# install.packages(\"randomForestSRC\")\n",
    "# install.packages(\"GGally\")\n",
    "options(warn=0)\n",
    "library(randomForestSRC)\n",
    "library(GGally)\n",
    "\n",
    "# An correlation graph to make sense of data variables as well as any relation that can effect train-label\n",
    "data(train, package = \"reshape\")\n",
    "ggpairs(\n",
    "  train[, c(2, 3, 4, 5, 8, 10)],\n",
    "  upper = list(continuous = \"density\", combo = \"box_no_facet\"),\n",
    "  lower = list(continuous = \"points\", combo = \"dot_no_facet\")\n",
    ")\n",
    "\n",
    "# Creating extra variables that might represent fuel efficiency \n",
    "train$log.Eng.Displacement = log(train$Eng.Displacement)\n",
    "train$log.Gear = log(train$No.Gears)\n",
    "train$log.Max.Ethanol = log(train$Max.Ethanol)\n",
    "train$log.No.Cylinders = log(train$No.Cylinders)\n",
    "\n",
    "train$SQRT.Eng = sqrt(train$Eng.Displacement)\n",
    "train$SQRT.Gear = sqrt(train$No.Gears)\n",
    "train$SQRT.Max.Ethanol = sqrt(train$Max.Ethanol)\n",
    "train$SQRT.No.Cylinders = sqrt(train$No.Cylinders)\n",
    "\n",
    "train$Sq.Eng = (train$Eng.Displacement)^2\n",
    "train$Sq.Gear = (train$No.Gears)^2\n",
    "train$Sq.Max.Ethanol = (train$Max.Ethanol)^2\n",
    "train$Sq.No.Cylinders = (train$No.Cylinders)^2\n",
    "\n",
    "train$log2.Ethanol <- log(log(log(train$Max.Ethanol)))\n",
    "train$Combination <- train$log.Eng.Displacement * train$Sq.Max.Ethanol *train$Sq.Gear *train$Sq.No.Cylinders\n",
    "\n",
    "# For predicting the test label, new variables were introduced in test data\n",
    "test$log.Eng.Displacement = log(test$Eng.Displacement)\n",
    "test$log.Gear = log(test$No.Gears)\n",
    "test$log.Max.Ethanol = log(test$Max.Ethanol)\n",
    "test$log.No.Cylinders = log(test$No.Cylinders)\n",
    "\n",
    "test$SQRT.Eng = sqrt(test$Eng.Displacement)\n",
    "test$SQRT.Gear = sqrt(test$No.Gears)\n",
    "test$SQRT.Max.Ethanol = sqrt(test$Max.Ethanol)\n",
    "test$SQRT.No.Cylinders = sqrt(test$No.Cylinders)\n",
    "\n",
    "test$Sq.Eng = (test$Eng.Displacement)^2\n",
    "test$Sq.Gear = (test$No.Gears)^2\n",
    "test$Sq.Max.Ethanol = (test$Max.Ethanol)^2\n",
    "test$Sq.No.Cylinders = (test$No.Cylinders)^2\n",
    "\n",
    "test$log2.Ethanol <- log(log(log(test$Max.Ethanol)))\n",
    "test$Combination <- test$log.Eng.Displacement * test$Sq.Max.Ethanol *test$Sq.Gear *test$Sq.No.Cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to check the performance of your model\n",
    "rmse <- function(pred.label, truth.label){\n",
    "    # Lower is better\n",
    "    return(sqrt(mean((pred.label - truth.label)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your final model here, use additional coding block if you want to\n",
    "fin.mod <- NULL\n",
    "# Creating an model with tuned variables using experiments\n",
    "fin.mod <- rfsrc(Comb.FE ~ Eng.Displacement + Eng.Displacement * log.Eng.Displacement + Combination + log2.Ethanol + Aspiration + No.Gears * \n",
    "                   Lockup.Torque.Converter + Drive.Sys + Max.Ethanol + Fuel.Type + No.Cylinders +\n",
    "                   log.Eng.Displacement * log.Gear * log.Max.Ethanol * log.No.Cylinders + SQRT.Eng * SQRT.Gear *\n",
    "                   SQRT.Max.Ethanol * SQRT.No.Cylinders + Sq.Eng * Sq.Gear * Sq.Max.Ethanol * Sq.No.Cylinders,\n",
    "    data = train,importance=TRUE,proximity=TRUE,mtry=8,nodesize=2,ntree=950,replace=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using any packages that perform the prediction differently, please change the value of this variable\n",
    "pred.label <- predict(fin.mod, test)\n",
    "predict.label <- pred.label$predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# put this label in a csv file to commit to the Leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(predict.label)), \"Prediction\" = predict.label),  \n",
    "          \"RegressionPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "RMSE.fin <- rmse(pred.label, label$Label)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "<h1>Part 2: Classification (50 Marks)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to work with \"Census Income Dataset\" which was originally donated by Ronny Kohavi and Barry Becker to UCI (University of California, Irvine) in 1996. This is a trimmed dataset used for machine learning students to study classification. \n",
    "\n",
    "This dataset has collected over 40,000 records (we excluded some data in our version) regarding personal yearly income with 12 attributes (predictors). The attributes comprise many aspects of a person that may contribute to the yearly income. You can use summary() function to obtain the attributes information. Your prediction task is to determine whether a person makes over 50K a year.\n",
    "\n",
    "We have splitted the dataset into a trainning and a testing set. There are 27245 records in the training set while 13631 records in the testing set. Besides the 12 predictors, there is one more column named Salary indicating whether a person's yearly income is over 50K. The label information is a seperated file for the testing set and will be used by us to asess your performance later. Note the label TRUE means an individual's yearly salary exceeds 50K while FALSE means an individual's yearly salary is under 50K.\n",
    "\n",
    "$\\textbf{Note:}$ If not explicitly mentioned, libraries are not allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"ClassTrain.csv\")\n",
    "test  <- read.csv(\"ClassTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/ClassTrain.csv\")\n",
    "test  <- read.csv(\"../data/ClassTest.csv\")\n",
    "label <- read.csv(\"../data/ClassTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "<h2> Question 1 (10 Marks) </h2>\n",
    "\n",
    "Fit a $\\textbf{Generalized Linear Model (Logistic Regression)}$ to predict level of income (salary) $\\left(\\;\\geq 50\\;\\text{K, or } <50\\;\\text{K}\\;\\right)$ using the ``train`` dataset. Using the results of fitting this model, which predictors do you think are possibly associated with the level of Salary (use ``0.05`` significant level), and why? Which ``three variables`` appear to be the strongest predictors of salary, and why? \n",
    "\n",
    "Furthermore, you can see that you have much more predictors in this part than in the ``linear model`` from Part 1 $\\Rightarrow$ manually checking information is counterproductive. Thus, please write a function to automate these processes $\\textbf{(1)}$ selecting important feature against 0.05 threshold and $\\textbf{(2)}$ Selecting three most important features.\n",
    "\n",
    "$\\textbf{Note}$: You don't have to worry about categorical variables here since R can deal with this automatically, focus your efforts on interpretation. Additionally, when explaining why features are strongly associated with the target, please refrain from giving one or two sentences answers, these answers are not descriptive and will result in a deduction of marks. Finally, please name the model here ``glm.fit`` and have the parameter in the model set to ``family = binomial``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 1:</span>__ \n",
    "\n",
    "__<span style=\"color:blue\">Predictor/variables that have possibly associated with Salary are:- </span>__\n",
    "\n",
    "Age, WorkClass, FinalWeight, Education, MaritalStatus, Occupation, Relationship, Gender, CapitalGain, CapitalLoss, HoursWork\n",
    "\n",
    "\n",
    "__<span style=\"color:blue\">Predictor/variables with strongest associated with fuel efficiency are:- </span>__\n",
    "\n",
    "1. CapitalGain\n",
    "2. HoursWork\n",
    "3. CapitalLoss\n",
    "\n",
    "These are the most significant and have the least $\\textbf{Pr(>|z|)}$ value when we apply Generalized linear modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ ., family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.1013  -0.5296  -0.1926   0.0276   3.4349  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.614e+00  4.525e-01 -16.826  < 2e-16 ***\n",
       "Age                                 2.626e-02  1.779e-03  14.762  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.214e-01  1.168e-01  -6.179 6.46e-10 ***\n",
       "WorkClassPrivate                   -4.734e-01  9.693e-02  -4.884 1.04e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.974e-01  1.283e-01  -2.317 0.020506 *  \n",
       "WorkClassSelf-emp-not-inc          -9.994e-01  1.139e-01  -8.772  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.757e-01  1.294e-01  -5.996 2.03e-09 ***\n",
       "FinalWeight                         7.896e-07  1.822e-07   4.334 1.46e-05 ***\n",
       "Education11th                       6.909e-02  2.201e-01   0.314 0.753589    \n",
       "Education12th                       5.005e-01  2.940e-01   1.702 0.088676 .  \n",
       "Education7th-8th                   -6.213e-01  2.592e-01  -2.397 0.016530 *  \n",
       "Education9th                       -2.472e-01  2.856e-01  -0.865 0.386877    \n",
       "EducationAssoc-acdm                 1.302e+00  1.843e-01   7.066 1.60e-12 ***\n",
       "EducationAssoc-voc                  1.263e+00  1.772e-01   7.127 1.02e-12 ***\n",
       "EducationBachelors                  1.931e+00  1.647e-01  11.724  < 2e-16 ***\n",
       "EducationDoctorate                  3.076e+00  2.380e-01  12.926  < 2e-16 ***\n",
       "EducationHS-grad                    7.790e-01  1.598e-01   4.874 1.09e-06 ***\n",
       "EducationMasters                    2.319e+00  1.767e-01  13.126  < 2e-16 ***\n",
       "EducationProf-school                2.874e+00  2.145e-01  13.396  < 2e-16 ***\n",
       "EducationSome-college               1.108e+00  1.622e-01   6.832 8.36e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.345e+00  3.050e-01   7.687 1.51e-14 ***\n",
       "MaritalStatusMarried-spouse-absent -3.345e-02  2.697e-01  -0.124 0.901286    \n",
       "MaritalStatusNever-married         -4.513e-01  9.187e-02  -4.912 9.01e-07 ***\n",
       "MaritalStatusSeparated             -9.621e-02  1.733e-01  -0.555 0.578829    \n",
       "MaritalStatusWidowed                1.484e-01  1.656e-01   0.896 0.370163    \n",
       "OccupationCraft-repair              5.906e-02  8.379e-02   0.705 0.480884    \n",
       "OccupationExec-managerial           7.693e-01  8.089e-02   9.511  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.919e-01  1.457e-01  -6.805 1.01e-11 ***\n",
       "OccupationHandlers-cleaners        -7.641e-01  1.529e-01  -4.999 5.77e-07 ***\n",
       "OccupationMachine-op-inspct        -2.794e-01  1.073e-01  -2.605 0.009191 ** \n",
       "OccupationOther-service            -8.967e-01  1.300e-01  -6.900 5.22e-12 ***\n",
       "OccupationProf-specialty            4.654e-01  8.613e-02   5.403 6.54e-08 ***\n",
       "OccupationProtective-serv           6.229e-01  1.302e-01   4.784 1.72e-06 ***\n",
       "OccupationSales                     2.770e-01  8.625e-02   3.211 0.001322 ** \n",
       "OccupationTech-support              6.359e-01  1.159e-01   5.488 4.07e-08 ***\n",
       "OccupationTransport-moving         -1.027e-01  1.032e-01  -0.995 0.319541    \n",
       "RelationshipNot-in-family           6.652e-01  3.021e-01   2.202 0.027683 *  \n",
       "RelationshipOther-relative         -4.067e-01  2.918e-01  -1.394 0.163395    \n",
       "RelationshipOwn-child              -6.044e-01  2.943e-01  -2.054 0.039994 *  \n",
       "RelationshipUnmarried               5.707e-01  3.174e-01   1.798 0.072185 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.071  < 2e-16 ***\n",
       "RaceAsian-Pac-Islander              9.879e-01  2.997e-01   3.296 0.000979 ***\n",
       "RaceBlack                           3.929e-01  2.427e-01   1.619 0.105400    \n",
       "RaceOther                           1.524e-01  4.397e-01   0.347 0.728862    \n",
       "RaceWhite                           5.396e-01  2.305e-01   2.340 0.019266 *  \n",
       "GenderMale                          8.679e-01  8.403e-02  10.328  < 2e-16 ***\n",
       "CapitalGain                         3.191e-04  1.107e-05  28.830  < 2e-16 ***\n",
       "CapitalLoss                         6.503e-04  3.999e-05  16.264  < 2e-16 ***\n",
       "HoursWork                           2.965e-02  1.774e-03  16.714  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17976  on 27196  degrees of freedom\n",
       "AIC: 18074\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build your model, keep family = binomial, ignore the warnings, they are benign\\\n",
    "# mutlivariate generalized linear regression model\n",
    "glm.fit <- glm(Salary ~.,data = train, family = binomial)\n",
    "options(warn=0)\n",
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CapitalGain \n",
      "9.086632e-183 \n",
      "   HoursWork \n",
      "1.033315e-62 \n",
      " CapitalLoss \n",
      "1.791718e-59 \n"
     ]
    }
   ],
   "source": [
    "TopPredictor <- function(model){\n",
    "    # Finding the best predictors/variables for fuel efficiency\n",
    "    Predictor <- coef(summary(model))\n",
    "    # Filtering only predictors/variables with significance of 0.05\n",
    "    Best.Pred <- Predictor[Predictor[,\"Pr(>|z|)\"]<0.05,]\n",
    "    Predictor <- sort(Best.Pred[,4],decreasing=FALSE)\n",
    "    print(Predictor[1])\n",
    "    print(Predictor[3])\n",
    "    print(Predictor[4])\n",
    "    }\n",
    "\n",
    "TopPredictor(glm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "<h2> Question 2 (10 Marks) </h2>\n",
    "\n",
    "Firstly, please use the model created in the previous question to predict for the labels of the $\\textbf{train}$ data. Consequently, our objective is to compare this ``predict.label`` with the ``truth.label`` from the $\\textbf{test}$ data. However, as we don't know the $\\textbf{test}$ label, we have to estimate model performance using $\\textbf{train}$ data at this moment.\n",
    "\n",
    "Secondly, since our objective is to estimate the performance of this model in making correct predictions; thus, this question also asks you to explore different [performance metrics](https://en.wikipedia.org/wiki/Precision_and_recall) for classification models. The metrics we will use are $\\textbf{Accuracy, Precision, Recall, and F1 Score}$, please create a function to calculate these value and print them out properly using the given structure.\n",
    "\n",
    "Additionally, please also discuss the results of these values in the context of your model.\n",
    "\n",
    "$\\textbf{Note}$: This asks for your descriptions, please refrain from using one or two lines to describe/discuss the effect. Keep answers to be 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 2:</span>__ \n",
    "\n",
    "Accuracy measured using the true postive and true negative values. After looking at the confusion matrix value, we find that the true negative values are indeed very high as compared to the true positive value. But still my accuracy is high. \n",
    "\n",
    "The value of precision, recall and f1-score the value is low in this confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your previous model to perform prediction, keep type = \"response\"\n",
    "# Don't worry if you receive some warnings, they are benign\n",
    "predict.label <- predict(glm.fit,train,type = \"response\")\n",
    "\n",
    "# Truth label from train data\n",
    "truth.label <- train$Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics function\n",
    "mod.stat <- function(predict.label, truth.label){\n",
    "    # instantiate the variables\n",
    "    accuracy <- NULL\n",
    "    precision <- NULL\n",
    "    recall <- NULL\n",
    "    F1 <- NULL\n",
    "    \n",
    "    ##############################\n",
    "    #Your calculatation here\n",
    "\n",
    "    # Creating an confusion table. Predicted probabilities which are > 0.5 as true and vice-versa\n",
    "    predict.label <- table(truth.label, predict.label > 0.5)\n",
    "    \n",
    "    accuracy <- round(sum(diag(predict.label)) / sum(predict.label),4)\n",
    "    precision <- round(predict.label[2,2] / (predict.label[2,2] + predict.label[1,2]),4)\n",
    "    recall <- round(predict.label[2,2] / (predict.label[2,2] + predict.label[2,1]),4)\n",
    "    F1 <- 2 * round(((precision * recall) / (precision + recall)),4)\n",
    "    \n",
    "    \n",
    "    ##############################\n",
    "    \n",
    "    # Return a list of value\n",
    "    return(list(\"accuracy\" = accuracy, \"precision\" = precision, \"recall\" = recall, \"fscore\" = F1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8452</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7395</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6107</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.669</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8452\n",
       "\\item[\\$precision] 0.7395\n",
       "\\item[\\$recall] 0.6107\n",
       "\\item[\\$fscore] 0.669\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8452\n",
       "$precision\n",
       ":   0.7395\n",
       "$recall\n",
       ":   0.6107\n",
       "$fscore\n",
       ":   0.669\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8452\n",
       "\n",
       "$precision\n",
       "[1] 0.7395\n",
       "\n",
       "$recall\n",
       "[1] 0.6107\n",
       "\n",
       "$fscore\n",
       "[1] 0.669\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod.stat(predict.label, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "<h2> Question 3 (5 Marks) </h2>\n",
    "\n",
    "Use the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially unimportant variables. Checking the performance of your model using the created ``mod.stat()`` function, please give your discussion as how this model is compared with the ``glm.fit``(you can run the ``mod.stat()`` function for this as well if you want to).\n",
    "\n",
    "$\\textbf{Note}$: please don't change the default direction ``both`` in the step function, this is so that we can check your work easily. Additionally, please name this model ``sw.fit``. Don't worry about the warnings, they are benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ybm3VKDOWJe"
   },
   "source": [
    "__<span style=\"color:blue\">SOLUTION 3:</span>__ \n",
    "\n",
    "From the data we can see that sw.fit has slightly higher precision in comparision to glm.fit. however sw.fit has lower F1 and recall score in comparision to glm.fit. sw.fit and glm.fit don't have any significat improvement over each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ Age + WorkClass + FinalWeight + Education + \n",
       "    MaritalStatus + Occupation + Relationship + Gender + CapitalGain + \n",
       "    CapitalLoss + HoursWork, family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.0961  -0.5291  -0.1936   0.0279   3.4393  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.130e+00  3.929e-01 -18.146  < 2e-16 ***\n",
       "Age                                 2.644e-02  1.778e-03  14.869  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.159e-01  1.163e-01  -6.154 7.54e-10 ***\n",
       "WorkClassPrivate                   -4.588e-01  9.626e-02  -4.766 1.88e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.776e-01  1.278e-01  -2.173  0.02977 *  \n",
       "WorkClassSelf-emp-not-inc          -9.857e-01  1.133e-01  -8.703  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.653e-01  1.290e-01  -5.930 3.02e-09 ***\n",
       "FinalWeight                         7.496e-07  1.800e-07   4.164 3.13e-05 ***\n",
       "Education11th                       7.190e-02  2.201e-01   0.327  0.74395    \n",
       "Education12th                       5.065e-01  2.939e-01   1.724  0.08479 .  \n",
       "Education7th-8th                   -6.220e-01  2.593e-01  -2.399  0.01643 *  \n",
       "Education9th                       -2.417e-01  2.851e-01  -0.848  0.39663    \n",
       "EducationAssoc-acdm                 1.323e+00  1.841e-01   7.187 6.60e-13 ***\n",
       "EducationAssoc-voc                  1.277e+00  1.770e-01   7.215 5.38e-13 ***\n",
       "EducationBachelors                  1.947e+00  1.645e-01  11.838  < 2e-16 ***\n",
       "EducationDoctorate                  3.089e+00  2.377e-01  12.998  < 2e-16 ***\n",
       "EducationHS-grad                    7.881e-01  1.596e-01   4.937 7.95e-07 ***\n",
       "EducationMasters                    2.337e+00  1.765e-01  13.239  < 2e-16 ***\n",
       "EducationProf-school                2.894e+00  2.145e-01  13.495  < 2e-16 ***\n",
       "EducationSome-college               1.117e+00  1.621e-01   6.893 5.47e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.357e+00  3.045e-01   7.743 9.75e-15 ***\n",
       "MaritalStatusMarried-spouse-absent -3.049e-02  2.690e-01  -0.113  0.90977    \n",
       "MaritalStatusNever-married         -4.482e-01  9.172e-02  -4.886 1.03e-06 ***\n",
       "MaritalStatusSeparated             -1.101e-01  1.728e-01  -0.637  0.52405    \n",
       "MaritalStatusWidowed                1.497e-01  1.655e-01   0.904  0.36583    \n",
       "OccupationCraft-repair              6.350e-02  8.372e-02   0.759  0.44812    \n",
       "OccupationExec-managerial           7.726e-01  8.083e-02   9.558  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.869e-01  1.456e-01  -6.779 1.21e-11 ***\n",
       "OccupationHandlers-cleaners        -7.678e-01  1.528e-01  -5.026 5.02e-07 ***\n",
       "OccupationMachine-op-inspct        -2.857e-01  1.072e-01  -2.665  0.00770 ** \n",
       "OccupationOther-service            -9.059e-01  1.298e-01  -6.981 2.92e-12 ***\n",
       "OccupationProf-specialty            4.656e-01  8.599e-02   5.414 6.15e-08 ***\n",
       "OccupationProtective-serv           6.192e-01  1.301e-01   4.760 1.94e-06 ***\n",
       "OccupationSales                     2.797e-01  8.618e-02   3.246  0.00117 ** \n",
       "OccupationTech-support              6.444e-01  1.157e-01   5.568 2.57e-08 ***\n",
       "OccupationTransport-moving         -1.082e-01  1.031e-01  -1.050  0.29391    \n",
       "RelationshipNot-in-family           6.761e-01  3.016e-01   2.242  0.02499 *  \n",
       "RelationshipOther-relative         -4.031e-01  2.920e-01  -1.381  0.16742    \n",
       "RelationshipOwn-child              -5.877e-01  2.935e-01  -2.002  0.04525 *  \n",
       "RelationshipUnmarried               5.744e-01  3.168e-01   1.813  0.06984 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.076  < 2e-16 ***\n",
       "GenderMale                          8.747e-01  8.401e-02  10.412  < 2e-16 ***\n",
       "CapitalGain                         3.184e-04  1.106e-05  28.784  < 2e-16 ***\n",
       "CapitalLoss                         6.509e-04  3.998e-05  16.281  < 2e-16 ***\n",
       "HoursWork                           2.968e-02  1.774e-03  16.735  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17992  on 27200  degrees of freedom\n",
       "AIC: 18082\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting to suppress warnings\n",
    "options(warn=-1)\n",
    "# Fit a stepwise model\n",
    "sw.fit <- step(glm.fit,trace = 0,k = log(nrow(train)), direction=\"both\")\n",
    "# Setting to suppress warnings\n",
    "options(warn=0)\n",
    "# Getting the summary to understand the result\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction using train data and view the statistics\n",
    "predict.label <- NULL\n",
    "predict.label <- predict(sw.fit,train,type = \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8451</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7396</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6104</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.6688</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8451\n",
       "\\item[\\$precision] 0.7396\n",
       "\\item[\\$recall] 0.6104\n",
       "\\item[\\$fscore] 0.6688\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8451\n",
       "$precision\n",
       ":   0.7396\n",
       "$recall\n",
       ":   0.6104\n",
       "$fscore\n",
       ":   0.6688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8451\n",
       "\n",
       "$precision\n",
       "[1] 0.7396\n",
       "\n",
       "$recall\n",
       "[1] 0.6104\n",
       "\n",
       "$fscore\n",
       "[1] 0.6688\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only run the below if you have labels, in your submission, this must be UNCOMMENTED\n",
    "mod.stat(predict.label, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROVIDE DISCUSSION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Question 4 (Libraries are allowed) (25 Marks) </h2>\n",
    "\n",
    "Similar to the first part, to simulate for a realistic modelling process, this question will be in the form of a competition among students to find out who has the best model.\n",
    "\n",
    "Thus, You will be graded by the performance of your model compared to your classmates', the better your model, the higher your score. Additionally, you need to write a short paragraph describing/documenting your thought process in this model building process ``(300 words)``. Note that this is to explain to us why you build your current model so that we can verify that you understand the model you build and not just copy from other people.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "``install.packages(\"some package\", repos='http://cran.us.r-project.org')``\n",
    "\n",
    "``library(\"some package\")``\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction, our suggestion is for you to use the standard ``R version 3.6.1``\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly.\n",
    "\n",
    "\n",
    "$20$ Marks for the model performance in the competition\n",
    "\n",
    "$5$ Marks for logically writing down the thought process in building the final model\n",
    "\n",
    "This is the [link](https://www.kaggle.com/t/1bdebc96607742dbaf47ab36cd3ae421) to the competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<span style=\"color:blue\">SOLUTION 4:</span>__ \n",
    "\n",
    "After pruning the data using BIC criterion, the Accuarcy has observed to be 0.84. Hence, to improve the accuracy (RMSE score) first we need an better model \n",
    "\n",
    "Few Models were tried such as SVM, h2o and train function but best non tuned accuracy was observed using ranger. Which uses randomForest techniques. Further randomForest modelling were used as final model as in comparision with other models tried, RandomForest resulted in better score when checked for output on kaggle. After that hyper parameter optimization were neccesary to extract best Accuarcy.\n",
    "\n",
    "After multiple experiments combination of parameters were found which result in best score so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Library used\n",
    "\n",
    "# install.packages(\"ranger\")\n",
    "library(ranger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your final model here, use additional coding block if you want to\n",
    "fin.mod <- NULL\n",
    "# An example would be use the previous model as your final one\n",
    "fin.mod <- ranger(Salary ~ .,data = train, mtry = 3,min.node.size = 4,num.tree = 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the predict label for the TEST data\n",
    "pred.label <- predict(fin.mod, test)\n",
    "pred.label <- (pred.label$predictions > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Use this csv file to commit to the leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"ClassPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "source(\"../data/modassess.r\")\n",
    "model.perf <- mod.stat.test(pred.label,label$Label)\n",
    "print(model.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Va7-A1UviCmz"
   },
   "source": [
    "<h1>References</h1>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1 Instructions and Contents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
